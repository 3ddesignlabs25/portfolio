<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Troubleshooting | Michael Butler</title>
  <meta name="description" content="Help-desk-focused troubleshooting case studies documenting problem diagnosis, testing steps, and resolutions." />
  <link rel="stylesheet" href="styles.css" />
</head>

<body>
  <a class="skip-link" href="#main">Skip to content</a>

  <header class="site-header">
    <div class="container header-inner">
      <a class="brand" href="index.html" aria-label="Home">
        <span class="brand-mark" aria-hidden="true">MB</span>
        <span class="brand-text">
          <span class="brand-name">Michael Butler</span>
          <span class="brand-sub">Technical Support • Systems</span>
        </span>
      </a>

      <nav class="nav" aria-label="Primary">
        <a href="index.html">Home</a>
        <a href="projects.html">Projects</a>
        <a href="troubleshooting.html">Troubleshooting</a>
        <a href="resume.html">Resume</a>
        <a href="contact.html">Contact</a>
      </nav>
    </div>
  </header>

  <main id="main" class="main">
    <!-- Hero -->
    <section class="hero">
      <div class="container">
        <p class="kicker">Troubleshooting</p>

        <h1 class="headline">Problem Diagnosis & Resolution</h1>

        <p class="summary">
          This page documents real troubleshooting scenarios across hardware, software,
          and systems environments. Each case focuses on symptoms, isolation steps,
          testing, and verified outcomes—mirroring how issues are handled in technical
          support and IT roles.
        </p>
      </div>
    </section>

    <!-- Case Study -->
    <section class="section">
      <div class="container">
        <h2>Broken Python Virtual Environment (Raspberry Pi)</h2>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Problem</h2>
        <p>
          While working in a Raspberry Pi–based Python environment, I began encountering
          failures in scripts that had previously run without issue. These failures did
          not follow a clear pattern and often appeared after unrelated changes, such as
          installing new packages or restarting the system.
        </p>
        <p>
          At the time, it was not obvious what the root cause was. Error messages pointed
          to missing modules or import failures, but it was unclear whether the issue was
          caused by my code, the Python environment, or underlying system changes. In some
          cases, attempting one fix would appear to help, only for a different error to
          surface later.
        </p>
        <p>
          Debugging initially involved a degree of trial and error, driven by curiosity
          and testing assumptions rather than a complete understanding of what was
          happening internally. Through experimenting with environment activation,
          rebuilding virtual environments, and observing changes in behavior, patterns
          gradually became more visible.
        </p>
        <p>
          The objective became less about immediately identifying a precise root cause
          and more about restoring a predictable working environment while learning how
          different configuration changes affected execution. This experience highlighted
          the importance of isolation, documentation, and controlled experimentation when
          troubleshooting system-level issues.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Symptoms</h2>
        <ul>
          <li>
            Python scripts that had previously executed successfully began failing
            without any changes to the script files themselves.
          </li>
          <li>
            Import-related errors appeared intermittently, often indicating that
            modules were missing even though they had been installed earlier.
          </li>
          <li>
            Running the same script produced different results depending on how the
            session was started (new terminal session, SSH reconnect, or reboot).
          </li>
          <li>
            Activating the Python virtual environment appeared to succeed, but behavior
            suggested the system Python interpreter was sometimes still being used.
          </li>
          <li>
            Some issues temporarily resolved after rebuilding the environment or
            reinstalling packages, only to reappear later after additional changes.
          </li>
          <li>
            Error output was inconsistent and not always immediately actionable,
            making it difficult to distinguish between code issues and environment
            configuration problems.
          </li>
        </ul>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Diagnosis</h2>
        <p>
          Initial diagnosis focused on verifying whether the issue originated from
          outdated or mismatched dependencies. All installed Python packages within
          the virtual environment were reviewed and updated, and system packages were
          brought up to date to rule out known incompatibilities.
        </p>
        <p>
          When updating dependencies did not resolve the issue, the Python virtual
          environment was fully removed and recreated. This was done to eliminate the
          possibility of partial installs, corrupted state, or conflicts caused by
          previous experimentation.
        </p>
        <p>
          After environment resets failed to produce consistent results, attention
          shifted to the application code itself. Individual scripts were reviewed,
          modified, and in some cases temporarily removed to determine whether logic
          errors or import behavior within the codebase were contributing to the
          failures.
        </p>
        <p>
          As inconsistencies persisted, broader system-level troubleshooting was
          attempted. The Raspberry Pi was reimaged with a fresh operating system
          install, and the sandbox was rebuilt from scratch to establish a known
          baseline state.
        </p>
        <p>
          During this process, the setup script used to configure the environment was
          also examined and edited to check for mistakes, incomplete commands, or
          unexpected behavior that could have caused dependencies to install or
          configure incorrectly.
        </p>
        <p>
          Despite these efforts, the exact cause remained unclear. At this stage, the
          issue was escalated by seeking external input, which helped clarify how
          environment activation and interpreter selection were affecting execution.
          With this guidance, the problem became easier to isolate and resolve.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Resolution</h2>
        <p>
          The issue was ultimately stabilized by reestablishing a clean and predictable
          execution environment rather than identifying a single isolated failure.
          After reviewing interpreter paths and environment activation behavior, it
          became clear that inconsistent use of the system Python interpreter versus
          the virtual environment was contributing to the observed issues.
        </p>
        <p>
          A fresh Python virtual environment was created and verified explicitly by
          checking interpreter paths before running any scripts. Dependencies were
          reinstalled in a controlled order, and scripts were tested incrementally to
          confirm expected behavior at each step.
        </p>
        <p>
          The setup script was adjusted to ensure environment creation and activation
          steps were explicit and repeatable. This reduced ambiguity around which
          interpreter and package set were being used during execution.
        </p>
        <p>
          Once the environment was rebuilt and validated, script failures stopped
          occurring intermittently and behavior became consistent across sessions,
          reboots, and SSH connections. While the exact trigger that caused the initial
          instability could not be conclusively identified, enforcing stricter
          environment isolation and verification resolved the practical impact of the
          issue.
        </p>
        <p>
          The resolution reinforced the importance of treating environment state as a
          first-class component of troubleshooting, particularly when working across
          multiple tools, sessions, and system updates.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Lessons Learned</h2>
        <ul>
          <li>
            <strong>Environment isolation applies across languages.</strong><br />
            The same principles used to stabilize Python environments can be applied to
            other languages and toolchains. This reinforced that controlled setup,
            isolation, and rebuildability are transferable skills rather than Python-
            specific solutions.
          </li>
          <li>
            <strong>Tooling must match the interpreter or runtime.</strong><br />
            Installing packages, libraries, or tools without confirming they align with
            the active interpreter or runtime leads to subtle and difficult-to-diagnose
            failures. Dependencies must be installed intentionally for the environment
            that will actually execute the code.
          </li>
          <li>
            <strong>Mismatched environments cause silent breakage.</strong><br />
            Systems do not always fail loudly when components are misaligned. Scripts may
            partially work or fail inconsistently, making verification of execution
            context just as important as code correctness.
          </li>
          <li>
            <strong>Stability comes from discipline, not complexity.</strong><br />
            The most effective improvements came from simplifying workflows, validating
            assumptions, and rebuilding cleanly rather than layering additional fixes on
            top of an unstable system.
          </li>
        </ul>
      </div>
    </section>

    <!-- Case Study -->
    <section class="section">
      <div class="container">
        <h2>CyberPie Field Node: Network Bring-Up & Service Reliability (Pi Zero 2 W)</h2>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Problem</h2>
        <p>
          While building CyberPie as a service-first field node, I ran into reliability issues during initial setup:
          the device couldn’t consistently get internet access for updates and installs, and startup behavior wasn’t
          predictable after reboots. The goal was a repeatable “boot → network → services” flow.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Symptoms</h2>
        <ul>
          <li>Interface present but no internet connectivity</li>
          <li>System couldn’t obtain a DHCP lease on some boots/configurations</li>
          <li>Expected network tooling missing on minimal images</li>
          <li>Services failing at boot because networking wasn’t ready yet</li>
        </ul>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Diagnosis</h2>
        <p>
          I treated this like a support ticket: confirm link state first, confirm interface config, confirm which network
          stack is installed/enabled, and confirm DHCP client availability. That prevented chasing DNS or “app bugs” when
          the real issue was the underlying network tooling and startup order.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Resolution</h2>
        <ul>
          <li>Verified interface state and link before troubleshooting higher layers</li>
          <li>Installed/enabled the correct DHCP/network stack for the OS image in use</li>
          <li>Adjusted service dependencies so required services wait for network readiness when appropriate</li>
          <li>Validated the fix with multiple reboot tests to ensure it wasn’t a one-time “works until restart” outcome</li>
        </ul>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Lessons Learned</h2>
        <ul>
          <li><strong>Assume nothing about baseline tooling.</strong> Minimal images often omit utilities you expect.</li>
          <li><strong>Fix boot order, not just commands.</strong> Reliability comes from correct dependencies and startup flow.</li>
          <li><strong>Reboot tests are non-negotiable.</strong> If it only works once, it isn’t solved.</li>
        </ul>
      </div>
    </section>

    <!-- Case Study -->
    <section class="section">
      <div class="container">
        <h2>Control Node / Security Console: Package Manager & Service Setup (Pi 5)</h2>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Problem</h2>
        <p>
          During setup of the Pi 5 control node, installs and updates were blocked by package manager state issues and
          service configuration mismatches. The goal was to restore a clean update path and enable core services reliably
          (e.g., SSH) in a way that survives reboots.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Symptoms</h2>
        <ul>
          <li>Package installs failing due to interrupted dpkg state</li>
          <li>Update errors caused by stale/incorrect repository sources</li>
          <li>Service enable commands failing because unit names didn’t match the OS/service layout</li>
        </ul>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Diagnosis</h2>
        <p>
          I separated the issue into layers: (1) restore package manager consistency, (2) correct repository sources so
          updates run securely, then (3) verify the correct packages are installed and the service unit names are accurate.
          This avoids “fixing symptoms” while the system is still in a broken state.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Resolution</h2>
        <ul>
          <li>Recovered dpkg consistency first (complete configuration before attempting additional installs)</li>
          <li>Removed/disabled invalid repository entries so updates could run securely</li>
          <li>Installed the correct service packages and enabled the correct units</li>
          <li>Confirmed persistence via reboot validation</li>
        </ul>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Lessons Learned</h2>
        <ul>
          <li><strong>Package manager health is priority zero.</strong> If apt/dpkg is inconsistent, everything else is noise.</li>
          <li><strong>Repo configuration is a security control.</strong> Secure update paths matter as much as functionality.</li>
          <li><strong>Service names aren’t universal.</strong> Always verify the actual unit and installed package.</li>
        </ul>
      </div>
    </section>

    <!-- Case Study -->
    <section class="section">
      <div class="container">
        <h2>Home Server Hosting: Website Deployment & Self-Hosting Troubleshooting</h2>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Problem</h2>
        <p>
          While preparing to self-host this portfolio and related web projects, I needed the environment to be reliable,
          secure, and maintainable. That meant having repeatable deploy steps, predictable service behavior, and a clean
          separation between “public demo mode” (GitHub Pages) and “self-hosted mode.”
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Symptoms</h2>
        <ul>
          <li>Site not reachable after configuration changes (service, firewall, or routing related)</li>
          <li>Permission/ownership issues after moving files and setting up deploy workflows</li>
          <li>Configuration drift between “it worked once” and “it survives reboots/updates”</li>
        </ul>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Diagnosis</h2>
        <p>
          I used a layered approach: confirm network reachability, confirm service status, inspect logs, validate file
          permissions, and then verify configuration correctness. Every attempted fix was validated through service status
          checks and log inspection to avoid guesswork.
        </p>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Resolution</h2>
        <ul>
          <li>Validated network path and service binding (reachable host + listening service)</li>
          <li>Used service management and logs to isolate failures quickly and repeatably</li>
          <li>Standardized file permissions/ownership for web roots and deploy directories</li>
          <li>Documented a repeatable deploy/update procedure to reduce configuration drift</li>
          <li>Kept GitHub Pages content demo-safe while planning a separate self-hosted deployment path</li>
        </ul>
      </div>
    </section>

    <section class="section">
      <div class="container">
        <h2>Lessons Learned</h2>
        <ul>
          <li><strong>Logs are the receipt.</strong> They show what actually failed and why.</li>
          <li><strong>Repeatability is defensive.</strong> Consistent deploy steps reduce misconfig risk.</li>
          <li><strong>Separate public and private surfaces.</strong> Demo content stays public; services stay controlled.</li>
        </ul>
      </div>
    </section>


  <footer class="site-footer">
    <div class="container footer-inner">
      <p class="muted">© <span id="year"></span> Michael Butler. Built for clarity and support roles.</p>
    </div>
  </footer>

  <script>
    document.getElementById("year").textContent = new Date().getFullYear();
  </script>
</body>
</html>
